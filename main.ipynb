{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QM9 Pro.\n",
    "Not sure what I’m supposed to call this file.  \n",
    "Plan is to keep this repo much neater than before.  \n",
    "Getting to work now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch\n",
    "import os\n",
    "from torch.nn import Module, Embedding, Linear, MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msharshe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10965c090>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'main.ipynb'\n",
    "wandb.login()\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch_geometric/data/dataset.py:242: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, pass `force_reload=True` explicitly to reload the dataset.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load in dataset\n",
    "dataset = QM9(root='QM9/')\n",
    "\n",
    "# 80/10/10 split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# build train, val, test datasets out of main dataset\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# turn into DataLoaders for batching efficiency\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = {\n",
    "    \"base_learning_rate\": 1e-3,\n",
    "    \"architecture\": \"Sparse 2-layer MPNN\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"dataset\": \"QM9\",\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"name\": \"2LP\"\n",
    "}\n",
    "\n",
    "config_2 = {\n",
    "    \"base_learning_rate\": 1e-3,\n",
    "    \"architecture\": \"Sparse 2-layer MPNN\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"CosineAnnealingWarmRestarts\",\n",
    "    \"dataset\": \"QM9\",\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"name\": \"2LC\"\n",
    "}\n",
    "\n",
    "config_3 = {\n",
    "    \"base_learning_rate\": 1e-3,\n",
    "    \"architecture\": \"Sparse 1-layer MPNN\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"dataset\": \"QM9\",\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"name\": \"1LP\"\n",
    "}\n",
    "\n",
    "config_4 = {\n",
    "    \"base_learning_rate\": 1e-3,\n",
    "    \"architecture\": \"Sparse 1-layer MPNN\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"CosineAnnealingWarmRestarts\",\n",
    "    \"dataset\": \"QM9\",\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"name\": \"1LC\"\n",
    "}\n",
    "\n",
    "configs = [config_1, config_2, config_3, config_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleLayerGCN(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # hard-coded here\n",
    "        # could have been a parameter but that\n",
    "        # would not make anything easier\n",
    "        self.emb_dim = 32\n",
    "        \n",
    "        # initialize layers\n",
    "        self.embedding = Embedding(118, self.emb_dim)\n",
    "        self.conv1 = GCNConv(self.emb_dim, self.emb_dim)\n",
    "        self.conv2 = GCNConv(self.emb_dim, self.emb_dim)\n",
    "        self.lin1 = torch.nn.Linear(self.emb_dim, 8)\n",
    "        self.lin2 = torch.nn.Linear(8, 1)\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, data):\n",
    "        # get relevant parts from data arg\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        # notes: use rbf: radial basis function to\n",
    "        # expand the edges d_ij -> [w_ij1, w_ij2,\n",
    "        # \\cdot , w_ijd]\n",
    "        \n",
    "        # initialize x\n",
    "        x = data.x\n",
    "\n",
    "        # embed x and put it through embedding and\n",
    "        # conv layers\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        \n",
    "        # put x through linear layers\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        # combine representations of all nodes\n",
    "        # into single graph-level prediction\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        \n",
    "        # return x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerGCN(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # lower embedding dimension for a smaller model\n",
    "        self.emb_dim = 8\n",
    "        \n",
    "        # initialize layers\n",
    "        self.embedding = Embedding(118, self.emb_dim)\n",
    "        self.conv = GCNConv(self.emb_dim, self.emb_dim)\n",
    "        self.lin = torch.nn.Linear(self.emb_dim, 1)\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, data):\n",
    "        # get relevant parts from data arg\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        \n",
    "        # initialize x\n",
    "        x = data.x\n",
    "\n",
    "        # put x through each layer\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv(x, edge_index, edge_attr)\n",
    "        x = self.lin(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        # combine into single output\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DoubleLayerGCN()\n",
    "model2 = DoubleLayerGCN()\n",
    "model3 = SingleLayerGCN()\n",
    "model4 = SingleLayerGCN()\n",
    "\n",
    "models = [model1, model2, model3, model4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/samharshe/Documents/Gerstein Lab/QM9 Pro/wandb/run-20240418_112057-vzorcvo0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sharshe/QM9-Pro-2/runs/vzorcvo0' target=\"_blank\">silver-fog-5</a></strong> to <a href='https://wandb.ai/sharshe/QM9-Pro-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sharshe/QM9-Pro-2' target=\"_blank\">https://wandb.ai/sharshe/QM9-Pro-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sharshe/QM9-Pro-2/runs/vzorcvo0' target=\"_blank\">https://wandb.ai/sharshe/QM9-Pro-2/runs/vzorcvo0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 OF 50 | VAL MEAN LOSS: 0.006441839504987001\n",
      "EPOCH 2 OF 50 | VAL MEAN LOSS: 0.006441826466470957\n",
      "EPOCH 3 OF 50 | VAL MEAN LOSS: 0.006441825069487095\n",
      "EPOCH 4 OF 50 | VAL MEAN LOSS: 0.006441824603825808\n",
      "EPOCH 5 OF 50 | VAL MEAN LOSS: 0.00644182413816452\n",
      "EPOCH 6 OF 50 | VAL MEAN LOSS: 0.00644182413816452\n",
      "EPOCH 7 OF 50 | VAL MEAN LOSS: 0.00644182413816452\n",
      "EPOCH 8 OF 50 | VAL MEAN LOSS: 0.00643862783908844\n",
      "EPOCH 9 OF 50 | VAL MEAN LOSS: 0.006438631098717451\n",
      "EPOCH 10 OF 50 | VAL MEAN LOSS: 0.006438631564378738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x105f19150>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 789, in _clean_thread_parent_frames\n",
      "    for identity in list(thread_to_parent_header.keys()):\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 11 OF 50 | VAL MEAN LOSS: 0.006438670679926872\n",
      "EPOCH 12 OF 50 | VAL MEAN LOSS: 0.006438683718442917\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# get actual and predicted values\u001b[39;00m\n\u001b[1;32m     41\u001b[0m y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\n\u001b[0;32m---> 42\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_hat, y)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mDoubleLayerGCN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index, edge_attr)\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[0;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# put x through linear layers\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "for config, model in zip(configs[0:1], models[0:1]):\n",
    "    # wandb project init\n",
    "    wandb.init(\n",
    "        project = \"QM9-Pro-2\",\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "    # hyperparameter init\n",
    "    num_epochs = config['epochs']\n",
    "    base_learning_rate = config['base_learning_rate']\n",
    "    loss_fn = MSELoss()\n",
    "    optimizer = Adam(model.parameters(), base_learning_rate)\n",
    "    \n",
    "    # define the scheduler dependig on config\n",
    "    if config['scheduler'] == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.1, patience=1, threshold=0)\n",
    "        # bool to control step() call below\n",
    "        plateau = True\n",
    "    else:\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=1, T_mult=2, eta_min=1e-5, last_epoch=-1)\n",
    "        # bool to control step() call below\n",
    "        plateau = False\n",
    "\n",
    "    # stats\n",
    "    val_mean_losses = []\n",
    "    test_losses = []\n",
    "    learning_rates = [base_learning_rate]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # TRAINING\n",
    "        # track gradients\n",
    "        model.train()\n",
    "        \n",
    "        i = 0\n",
    "        # loop through loader\n",
    "        for data in train_loader:\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get actual and predicted values\n",
    "            y = data.y\n",
    "            y_hat = model(data).view(-1)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            \n",
    "            # save loss every 100 goes\n",
    "            if i%100 == 0:\n",
    "                train_loss_item = loss.item()\n",
    "                wandb.log({\"train_losses\": train_loss_item})\n",
    "                \n",
    "                # save learning rate\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                wandb.log({\"training_rates\": lr})\n",
    "            i+=1\n",
    "            \n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # backprop\n",
    "            optimizer.step()\n",
    "        \n",
    "        # VAL\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # do not keep track of gradients\n",
    "        model.eval()\n",
    "        \n",
    "        # loop through val loader\n",
    "        for data in val_loader:\n",
    "            # get actual and predicted values\n",
    "            y = data.y\n",
    "            y_hat = model(data).view(-1)\n",
    "            \n",
    "            # calculate and save loss\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            \n",
    "            # save loss\n",
    "            epoch_loss_item = loss.item()\n",
    "            epoch_losses.append(epoch_loss_item)\n",
    "        \n",
    "        epoch_mean_loss = torch.mean(torch.tensor(epoch_losses)).item()\n",
    "        val_mean_losses.append(epoch_mean_loss)\n",
    "        wandb.log({\"epoch_mean_loss\": epoch_mean_loss})\n",
    "        \n",
    "        # print out the results of the epoch\n",
    "        print(f'EPOCH {epoch+1} OF {num_epochs} | VAL MEAN LOSS: {epoch_mean_loss}')\n",
    "        \n",
    "        # if this is our best val performance yet, save the weights\n",
    "        if min(val_mean_losses) == epoch_mean_loss:\n",
    "            torch.save(model, 'models/'+config['name']+'.pth')\n",
    "            \n",
    "        # if we are using a scheduler that needs the epoch loss passed in to know \n",
    "        # whether or not to change the LR, pass it in\n",
    "        if plateau:\n",
    "            scheduler.step(epoch_mean_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    \n",
    "    # TEST\n",
    "    for data in test_loader:\n",
    "        # get actual and predicted values\n",
    "        y = data.y\n",
    "        y_hat = model(data).view(-1)\n",
    "        \n",
    "        # calculate and save loss\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        \n",
    "        # save loss\n",
    "        test_losses.append(loss.item())\n",
    "    \n",
    "    # save and print mean test loss\n",
    "    test_mean_loss = torch.mean(torch.tensor(test_losses)).item()\n",
    "    wandb.log({\"test_mean_loss\": test_mean_loss})\n",
    "    print(f'TEST MEAN LOSS: {test_mean_loss}')\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 12.5%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_mean_loss</td><td>███████▁▁▁▁▁</td></tr><tr><td>train_losses</td><td>▄▄█▁▄█▁▄▆█▇▆█▇▆█▄▄▇▄▄▇▄▄█▁▄█▁▄█▁▇▆█▇▆█▇▆</td></tr><tr><td>training_rates</td><td>██████████████████████▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_mean_loss</td><td>0.00644</td></tr><tr><td>train_losses</td><td>0.00661</td></tr><tr><td>training_rates</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-fog-5</strong> at: <a href='https://wandb.ai/sharshe/QM9-Pro-2/runs/vzorcvo0' target=\"_blank\">https://wandb.ai/sharshe/QM9-Pro-2/runs/vzorcvo0</a><br/> View job at <a href='https://wandb.ai/sharshe/QM9-Pro-2/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NDAxNDAyNA==/version_details/v1' target=\"_blank\">https://wandb.ai/sharshe/QM9-Pro-2/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NDAxNDAyNA==/version_details/v1</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240418_112057-vzorcvo0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
