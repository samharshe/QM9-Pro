{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean absolute error calculation.\n",
    "I am stupid and did not allow any way of\n",
    "calculating the mean absolute error of my previous\n",
    "models after compressing the range of U_0.  \n",
    "\n",
    "I know this is hacky and gross. I just wanted to copy\n",
    "and paste from the main.ipynb with minimum pain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch\n",
    "import os\n",
    "from torch.nn import Module, Embedding, Linear, MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111b0c090>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'main.ipynb'\n",
    "wandb.login()\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch_geometric/data/dataset.py:242: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, pass `force_reload=True` explicitly to reload the dataset.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load in dataset\n",
    "dataset = QM9(root='QM9/')\n",
    "\n",
    "# 80/10/10 split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# build train, val, test datasets out of main dataset\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# turn into DataLoaders for batching efficiency\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = {\n",
    "    \"base_learning_rate\": 1e-3,\n",
    "    \"architecture\": \"Sparse 2-layer MPNN\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"dataset\": \"QM9\",\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"name\": \"2LP\"\n",
    "}\n",
    "\n",
    "config_2 = {\n",
    "    \"base_learning_rate\": 1e-3,\n",
    "    \"architecture\": \"Sparse 2-layer MPNN\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"CosineAnnealingWarmRestarts\",\n",
    "    \"dataset\": \"QM9\",\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"name\": \"2LC\"\n",
    "}\n",
    "\n",
    "config_3 = {\n",
    "    \"base_learning_rate\": 1e-3,\n",
    "    \"architecture\": \"Sparse 1-layer MPNN\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"dataset\": \"QM9\",\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"name\": \"1LP\"\n",
    "}\n",
    "\n",
    "config_4 = {\n",
    "    \"base_learning_rate\": 1e-3,\n",
    "    \"architecture\": \"Sparse 1-layer MPNN\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"CosineAnnealingWarmRestarts\",\n",
    "    \"dataset\": \"QM9\",\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"name\": \"1LC\"\n",
    "}\n",
    "\n",
    "configs = [config_1, config_2, config_3, config_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleLayerGCN(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # hard-coded here\n",
    "        # could have been a parameter but that\n",
    "        # would not make anything easier\n",
    "        self.emb_dim = 32\n",
    "        \n",
    "        # initialize layers\n",
    "        self.embedding = Embedding(118, self.emb_dim)\n",
    "        self.conv1 = GCNConv(self.emb_dim, self.emb_dim)\n",
    "        self.conv2 = GCNConv(self.emb_dim, self.emb_dim)\n",
    "        self.lin1 = torch.nn.Linear(self.emb_dim, 8)\n",
    "        self.lin2 = torch.nn.Linear(8, 1)\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, data):\n",
    "        # get relevant parts from data arg\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        # notes: use rbf: radial basis function to\n",
    "        # expand the edges d_ij -> [w_ij1, w_ij2,\n",
    "        # \\cdot , w_ijd]\n",
    "        \n",
    "        # initialize x\n",
    "        x = data.x\n",
    "\n",
    "        # embed x and put it through embedding and\n",
    "        # conv layers\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        \n",
    "        # put x through linear layers\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        \n",
    "        # combine representations of all nodes\n",
    "        # into single graph-level prediction\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        \n",
    "        # return x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DoubleLayerGCN()\n",
    "model2 = DoubleLayerGCN()\n",
    "model3 = DoubleLayerGCN()\n",
    "model4 = DoubleLayerGCN()\n",
    "\n",
    "models = [model1, model2, model3, model4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:smmjbnvf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 14.1%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_losses</td><td>█▆▂▁▇</td></tr><tr><td>training_rates</td><td>▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_losses</td><td>390.71371</td></tr><tr><td>training_rates</td><td>0.001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-waterfall-2</strong> at: <a href='https://wandb.ai/sharshe/QM9-Pro-MAE/runs/smmjbnvf' target=\"_blank\">https://wandb.ai/sharshe/QM9-Pro-MAE/runs/smmjbnvf</a><br/> View job at <a href='https://wandb.ai/sharshe/QM9-Pro-MAE/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NTU3MDM0MQ==/version_details/v1' target=\"_blank\">https://wandb.ai/sharshe/QM9-Pro-MAE/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NTU3MDM0MQ==/version_details/v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240422_203253-smmjbnvf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:smmjbnvf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/samharshe/Documents/Gerstein Lab/Deprecated/QM9 Pro/wandb/run-20240422_203420-4lm9mbo6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sharshe/QM9-Pro-MAE/runs/4lm9mbo6' target=\"_blank\">snowy-blaze-3</a></strong> to <a href='https://wandb.ai/sharshe/QM9-Pro-MAE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sharshe/QM9-Pro-MAE' target=\"_blank\">https://wandb.ai/sharshe/QM9-Pro-MAE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sharshe/QM9-Pro-MAE/runs/4lm9mbo6' target=\"_blank\">https://wandb.ai/sharshe/QM9-Pro-MAE/runs/4lm9mbo6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 OF 50 | VAL MEAN LOSS: 0.024614084511995316\n",
      "EPOCH 2 OF 50 | VAL MEAN LOSS: 0.02456079237163067\n",
      "EPOCH 3 OF 50 | VAL MEAN LOSS: 0.02441500872373581\n",
      "EPOCH 4 OF 50 | VAL MEAN LOSS: 0.024454772472381592\n",
      "EPOCH 5 OF 50 | VAL MEAN LOSS: 0.02442632056772709\n",
      "EPOCH 6 OF 50 | VAL MEAN LOSS: 0.02434210479259491\n",
      "EPOCH 7 OF 50 | VAL MEAN LOSS: 0.02431422285735607\n",
      "EPOCH 8 OF 50 | VAL MEAN LOSS: 0.02433185651898384\n",
      "EPOCH 9 OF 50 | VAL MEAN LOSS: 0.024322330951690674\n",
      "EPOCH 10 OF 50 | VAL MEAN LOSS: 0.024353045970201492\n",
      "EPOCH 11 OF 50 | VAL MEAN LOSS: 0.024546051397919655\n",
      "EPOCH 12 OF 50 | VAL MEAN LOSS: 0.024426430463790894\n",
      "EPOCH 13 OF 50 | VAL MEAN LOSS: 0.02441294677555561\n",
      "EPOCH 14 OF 50 | VAL MEAN LOSS: 0.02422128990292549\n",
      "EPOCH 15 OF 50 | VAL MEAN LOSS: 0.024204600602388382\n",
      "EPOCH 16 OF 50 | VAL MEAN LOSS: 0.024264555424451828\n",
      "EPOCH 17 OF 50 | VAL MEAN LOSS: 0.024290138855576515\n",
      "EPOCH 18 OF 50 | VAL MEAN LOSS: 0.024254510179162025\n",
      "EPOCH 19 OF 50 | VAL MEAN LOSS: 0.024203935638070107\n",
      "EPOCH 20 OF 50 | VAL MEAN LOSS: 0.024211248382925987\n",
      "EPOCH 21 OF 50 | VAL MEAN LOSS: 0.024159563705325127\n",
      "EPOCH 22 OF 50 | VAL MEAN LOSS: 0.024176498875021935\n",
      "EPOCH 23 OF 50 | VAL MEAN LOSS: 0.02430407889187336\n",
      "EPOCH 24 OF 50 | VAL MEAN LOSS: 0.02451072447001934\n",
      "EPOCH 25 OF 50 | VAL MEAN LOSS: 0.02455293759703636\n",
      "EPOCH 26 OF 50 | VAL MEAN LOSS: 0.024106252938508987\n",
      "EPOCH 27 OF 50 | VAL MEAN LOSS: 0.024068469181656837\n",
      "EPOCH 28 OF 50 | VAL MEAN LOSS: 0.024057701230049133\n",
      "EPOCH 29 OF 50 | VAL MEAN LOSS: 0.024060500785708427\n",
      "EPOCH 30 OF 50 | VAL MEAN LOSS: 0.024040868505835533\n",
      "EPOCH 31 OF 50 | VAL MEAN LOSS: 0.024056987836956978\n",
      "EPOCH 32 OF 50 | VAL MEAN LOSS: 0.02410515770316124\n",
      "EPOCH 33 OF 50 | VAL MEAN LOSS: 0.024126313626766205\n",
      "EPOCH 34 OF 50 | VAL MEAN LOSS: 0.024082308635115623\n",
      "EPOCH 35 OF 50 | VAL MEAN LOSS: 0.024082079529762268\n",
      "EPOCH 36 OF 50 | VAL MEAN LOSS: 0.02408779412508011\n",
      "EPOCH 37 OF 50 | VAL MEAN LOSS: 0.024130800738930702\n",
      "EPOCH 38 OF 50 | VAL MEAN LOSS: 0.02408618852496147\n",
      "EPOCH 39 OF 50 | VAL MEAN LOSS: 0.024066172540187836\n",
      "EPOCH 40 OF 50 | VAL MEAN LOSS: 0.0241191778331995\n",
      "EPOCH 41 OF 50 | VAL MEAN LOSS: 0.024069655686616898\n",
      "EPOCH 42 OF 50 | VAL MEAN LOSS: 0.024069948121905327\n",
      "EPOCH 43 OF 50 | VAL MEAN LOSS: 0.0240640826523304\n",
      "EPOCH 44 OF 50 | VAL MEAN LOSS: 0.023995114490389824\n",
      "EPOCH 45 OF 50 | VAL MEAN LOSS: 0.024078967049717903\n",
      "EPOCH 46 OF 50 | VAL MEAN LOSS: 0.023984825238585472\n",
      "EPOCH 47 OF 50 | VAL MEAN LOSS: 0.024282747879624367\n",
      "EPOCH 48 OF 50 | VAL MEAN LOSS: 0.02406982332468033\n",
      "EPOCH 49 OF 50 | VAL MEAN LOSS: 0.02414659596979618\n",
      "EPOCH 50 OF 50 | VAL MEAN LOSS: 0.024296453222632408\n",
      "TEST MEAN LOSS: 327.58447265625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_mean_loss</td><td>█▇▆▆▅▅▅▅▇▆▆▄▄▄▄▃▃▃▅▇▂▂▂▂▂▂▃▂▂▃▂▂▂▂▂▁▁▄▂▄</td></tr><tr><td>test_mean_loss</td><td>▁</td></tr><tr><td>train_losses</td><td>▁▂▂▅▂▂▅▆▆▇▆▆▇▆█▅▁█▅▁▂▂▄▂▂▄▂▆▇▆▆▇▆█▅▁█▅▁█</td></tr><tr><td>training_rates</td><td>██▄█▄▂██▆▄▃▁███▇▆▆▄▄▃▃▂▁▁█████▇▇▇▆▆▅▅▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_mean_loss</td><td>0.0243</td></tr><tr><td>test_mean_loss</td><td>327.58447</td></tr><tr><td>train_losses</td><td>0.02357</td></tr><tr><td>training_rates</td><td>0.00041</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-blaze-3</strong> at: <a href='https://wandb.ai/sharshe/QM9-Pro-MAE/runs/4lm9mbo6' target=\"_blank\">https://wandb.ai/sharshe/QM9-Pro-MAE/runs/4lm9mbo6</a><br/> View job at <a href='https://wandb.ai/sharshe/QM9-Pro-MAE/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NTU3MDM0MQ==/version_details/v2' target=\"_blank\">https://wandb.ai/sharshe/QM9-Pro-MAE/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2NTU3MDM0MQ==/version_details/v2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240422_203420-4lm9mbo6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for config, model in zip(configs[1:2], models[1:2]):\n",
    "    # wandb project init\n",
    "    wandb.init(\n",
    "        project = \"QM9-Pro-MAE\",\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "    # hyperparameter init\n",
    "    num_epochs = config['epochs']\n",
    "    base_learning_rate = config['base_learning_rate']\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    optimizer = Adam(model.parameters(), base_learning_rate)\n",
    "    \n",
    "    # define the scheduler dependig on config\n",
    "    if config['scheduler'] == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.1, patience=1, threshold=0)\n",
    "        # bool to control step() call below\n",
    "        plateau = True\n",
    "    else:\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=1, T_mult=2, eta_min=1e-5, last_epoch=-1)\n",
    "        # bool to control step() call below\n",
    "        plateau = False\n",
    "\n",
    "    # stats\n",
    "    val_mean_losses = []\n",
    "    test_losses = []\n",
    "    learning_rates = [base_learning_rate]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # TRAINING\n",
    "        # track gradients\n",
    "        model.train()\n",
    "        \n",
    "        i = 0\n",
    "        # loop through loader\n",
    "        for data in train_loader:\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get actual and predicted values\n",
    "            y = data.y\n",
    "            y_hat = model(data).view(-1)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            \n",
    "            # save loss every 100 goes\n",
    "            if i%100 == 0:\n",
    "                train_loss_item = loss.item()\n",
    "                wandb.log({\"train_losses\": train_loss_item})\n",
    "                \n",
    "                # save learning rate\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                wandb.log({\"training_rates\": lr})\n",
    "            i+=1\n",
    "            \n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # backprop\n",
    "            optimizer.step()\n",
    "        \n",
    "        # VAL\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # do not keep track of gradients\n",
    "        model.eval()\n",
    "        \n",
    "        # loop through val loader\n",
    "        for data in val_loader:\n",
    "            # get actual and predicted values\n",
    "            y = data.y\n",
    "            y_hat = model(data).view(-1)\n",
    "            \n",
    "            # calculate and save loss\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            \n",
    "            # save loss\n",
    "            epoch_loss_item = loss.item()\n",
    "            epoch_losses.append(epoch_loss_item)\n",
    "        \n",
    "        epoch_mean_loss = torch.mean(torch.tensor(epoch_losses)).item()\n",
    "        val_mean_losses.append(epoch_mean_loss)\n",
    "        wandb.log({\"epoch_mean_loss\": epoch_mean_loss})\n",
    "        \n",
    "        # print out the results of the epoch\n",
    "        print(f'EPOCH {epoch+1} OF {num_epochs} | VAL MEAN LOSS: {epoch_mean_loss}')\n",
    "        \n",
    "        # if this is our best val performance yet, save the weights\n",
    "        if min(val_mean_losses) == epoch_mean_loss:\n",
    "            torch.save(model, 'models/'+config['name']+'.pth')\n",
    "            \n",
    "        # if we are using a scheduler that needs the epoch loss passed in to know \n",
    "        # whether or not to change the LR, pass it in\n",
    "        if plateau:\n",
    "            scheduler.step(epoch_mean_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    \n",
    "    # TEST\n",
    "    for data in test_loader:\n",
    "        # get actual and predicted values\n",
    "        y = data.y\n",
    "        y_hat = model(data).view(-1)\n",
    "        \n",
    "        # calculate and save loss\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss *= 13388.7246\n",
    "        \n",
    "        # save loss\n",
    "        test_losses.append(loss.item())\n",
    "    \n",
    "    # save and print mean test loss\n",
    "    test_mean_loss = torch.mean(torch.tensor(test_losses)).item()\n",
    "    wandb.log({\"test_mean_loss\": test_mean_loss})\n",
    "    print(f'TEST MEAN LOSS: {test_mean_loss}')\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1085.5787)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(dataset.y * 13388.7246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model2.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6161"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
